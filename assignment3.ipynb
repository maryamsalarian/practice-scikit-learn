{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment (III)"
      ],
      "metadata": {
        "id": "MCFdTvKhveyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "##Please write your full name/names and student IDs here:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Full Name:\n",
        "*   Student ID:"
      ],
      "metadata": {
        "id": "FH_OnG1VvWch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Question 1. (3 points)\n",
        "* **For this question, please follow the steps outlined below:**\n",
        "## Part (a):\n",
        "\n",
        "* **Data Loading:** Read the lymph dataset from your mounted Google Drive using Pandas.\n",
        "\n",
        "* **Data Splitting:** Split the dataset randomly into a 60% training set and a 40% test set using scikit-learn's train_test_split function.\n",
        "\n",
        "* **Handling Categorical Data:** Encode categorical attributes with one-hot encoding using pandas.get_dummies, dropping the first attribute to avoid redundancy.\n",
        "\n",
        "* **Decision Tree Classifier (1st Experiment):** Train a Decision Tree classifier in scikit-learn on the training data, setting the splitting criterion to \"entropy,\" and requiring a minimum of 4 samples to split an internal node. Keep other settings unchanged.\n",
        "\n",
        "* **Reproducibility:** Set the random_state to 32 in train_test_split and DecisionTreeClassifier to make your codes reproducible.\n",
        "\n",
        "* **Performance Evaluation (1st Experiment):** Report the Decision Tree classifier's accuracy on the test data. Also, generate a high-resolution tree visualization (use plt.figure(figsize=(12, 8), dpi=300)).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "awdwTnyJB5E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/EECS4412/data/lymph.csv\")  # Load your dataset"
      ],
      "metadata": {
        "id": "l0JAmO5G10VD",
        "outputId": "1b1b84d2-04e1-4b02-d7fc-d6560b790f97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/data-mining-assignment3/datasets/lymph.csv\")  # Load your dataset\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/EECS4412/data/lymph.csv\")  # Load your dataset\n",
        "\n",
        "#.............................................................................\n",
        "# write the rest here\n",
        "df = pd.DataFrame(data)\n",
        "# check the raw data\n",
        "# print('first 5 rows:\\n', df.tail(5))\n",
        "# clean up the col name\n",
        "df.columns = df.columns.str.replace(\"'\", \"\").str.replace('\"', '').str.strip()\n",
        "# print('\\nolumns:', df.columns)\n",
        "# split the data, 40% test, 60% train\n",
        "X=df.drop(\"class\", axis=1)\n",
        "y=df['class']\n",
        "# try converting every column to numeric where possible\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
        "X_train = X_train.apply(pd.to_numeric, errors='ignore')\n",
        "X_test = X_test.apply(pd.to_numeric, errors='ignore')\n",
        "X_train.select_dtypes(include=['object']).columns\n",
        "# one-hot encoding\n",
        "encoded_df = pd.get_dummies(df, drop_first=True)\n",
        "# convert True and False to 1 and 0\n",
        "# print('\\nencoded table types', encoded_df.dtypes)\n",
        "encoded_df = encoded_df.replace({True: 1, False: 0})\n",
        "print('\\nencoded_df:\\n', encoded_df.tail(5))\n",
        "# the number of attr columns will increase, since each encoded attr value will get its separate col\n",
        "print('\\nencoded_df columns:\\n', encoded_df.columns)\n",
        "# TODO: resolve the error\n",
        "# using Decision Tree Classifier, with criterion=entropy, and min_samples_split=4\n",
        "clf = DecisionTreeClassifier(criterion='entropy', min_samples_split=4)\n",
        "# train the model\n",
        "# clf.fit(X_train, y_train)\n",
        "# # test the model\n",
        "# y_pred = clf.predict(X_test)\n",
        "# # calculate the accuracy score\n",
        "# accuracy = accuracy_score(y_pred)\n",
        "# print('\\naccuracy score is:', accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#.............................................................................\n"
      ],
      "metadata": {
        "id": "q5ZLZA2L9rHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76552cab-1bff-4112-aa9a-f9488426917d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "encoded_df:\n",
            "      lym_nodes_dimin  lym_nodes_enlar  no_of_nodes_in  lymphatics_deformed  \\\n",
            "143                1                3               2                    0   \n",
            "144                1                4               6                    1   \n",
            "145                1                2               4                    1   \n",
            "146                1                3               3                    1   \n",
            "147                1                1               2                    0   \n",
            "\n",
            "     lymphatics_displaced  lymphatics_normal  block_of_affere_yes  \\\n",
            "143                     1                  0                    0   \n",
            "144                     0                  0                    0   \n",
            "145                     0                  0                    1   \n",
            "146                     0                  0                    1   \n",
            "147                     0                  0                    0   \n",
            "\n",
            "     bl_of_lymph_c_yes  bl_of_lymph_s_yes  by_pass_yes  ...  \\\n",
            "143                  0                  0            0  ...   \n",
            "144                  0                  0            0  ...   \n",
            "145                  0                  0            1  ...   \n",
            "146                  0                  0            0  ...   \n",
            "147                  0                  0            0  ...   \n",
            "\n",
            "     changes_in_stru_no  changes_in_stru_reticular  changes_in_stru_stripped  \\\n",
            "143                   0                          0                         0   \n",
            "144                   0                          0                         0   \n",
            "145                   0                          0                         0   \n",
            "146                   0                          0                         0   \n",
            "147                   0                          0                         0   \n",
            "\n",
            "     special_forms_no  special_forms_vesicles  dislocation_of_yes  \\\n",
            "143                 0                       0                   1   \n",
            "144                 0                       1                   1   \n",
            "145                 0                       0                   1   \n",
            "146                 0                       1                   1   \n",
            "147                 0                       0                   1   \n",
            "\n",
            "     exclusion_of_no_yes  class_malign_lymph  class_metastases  class_normal  \n",
            "143                    1                   0                 1             0  \n",
            "144                    1                   1                 0             0  \n",
            "145                    1                   1                 0             0  \n",
            "146                    1                   1                 0             0  \n",
            "147                    1                   0                 1             0  \n",
            "\n",
            "[5 rows x 35 columns]\n",
            "\n",
            "encoded_df columns:\n",
            " Index(['lym_nodes_dimin', 'lym_nodes_enlar', 'no_of_nodes_in',\n",
            "       'lymphatics_deformed', 'lymphatics_displaced', 'lymphatics_normal',\n",
            "       'block_of_affere_yes', 'bl_of_lymph_c_yes', 'bl_of_lymph_s_yes',\n",
            "       'by_pass_yes', 'extravasates_yes', 'regeneration_of_yes',\n",
            "       'early_uptake_in_yes', 'changes_in_lym_oval', 'changes_in_lym_round',\n",
            "       'defect_in_node_lac_margin', 'defect_in_node_lacunar',\n",
            "       'defect_in_node_no', 'changes_in_node_lac_margin',\n",
            "       'changes_in_node_lacunar', 'changes_in_node_no',\n",
            "       'changes_in_stru_diluted', 'changes_in_stru_drop_like',\n",
            "       'changes_in_stru_faint', 'changes_in_stru_grainy', 'changes_in_stru_no',\n",
            "       'changes_in_stru_reticular', 'changes_in_stru_stripped',\n",
            "       'special_forms_no', 'special_forms_vesicles', 'dislocation_of_yes',\n",
            "       'exclusion_of_no_yes', 'class_malign_lymph', 'class_metastases',\n",
            "       'class_normal'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1290679289.py:24: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
            "  X_train = X_train.apply(pd.to_numeric, errors='ignore')\n",
            "/tmp/ipython-input-1290679289.py:25: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
            "  X_test = X_test.apply(pd.to_numeric, errors='ignore')\n",
            "/tmp/ipython-input-1290679289.py:31: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  encoded_df = encoded_df.replace({True: 1, False: 0})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (b):\n",
        "\n",
        "\n",
        "Modify the minimum samples required for splitting to 32 and re-run the experiment. Report the accuracy of the Decision Tree classifier with the modified parameter and create a visualization of the tree. Discuss the differences between the two Decision Trees and their respective results, highlighting how changing the minimum samples for splitting affects the tree structure and performance.\n"
      ],
      "metadata": {
        "id": "XYVoumdcMaP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#.............................................................................\n",
        "# write the rest here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#............................................................................."
      ],
      "metadata": {
        "id": "WoSuBJPxMc6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Put your discussion here**\n",
        "\n",
        "\n",
        "Discuss here.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xWMoMrePY4Rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Question 2. (5 points)\n",
        "##Part (a):\n",
        "*  Write a Python function named \"preprocess\" that preprocesses and prepares\n",
        "datasets for machine learning. It must take a training dataset and an optional test dataset as input. The code must first identify categorical and numerical attributes, then handle missing values by imputing the mean for numerical attributes and the most frequent value for categorical attributes. It is worth noting that to determine if an attribute is numerical, the process should involve checking its data type and potentially examining the number of unique values. This is necessary because an attribute may appear to have an \"object\" data type but could, in fact, be numerical in nature.\n",
        "\n",
        "*  Subsequently, the \"preprocess\" code must standardize the numerical attributes by removing the mean and scaling to unit variance., and encodes categorical attributes using one-hot encoding.\n",
        "\n",
        "*  If a test dataset is provided, it must undergo the same preprocessing steps, ensuring that the imputation for missing values and data scaling are performed based on the values obtained from the training dataset, thus maintaining consistency between the two datasets.\n",
        "\n",
        "*  At the end, the function must return the preprocessed training and test datasets as (X_train, y_train, X_test, y_test) tuple if the test dataset exists. Otherwise it must return (X_train, y_train). X stands for independent processed attributes, while y indicates the class attribute."
      ],
      "metadata": {
        "id": "siG8VbMXIbmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "def preprocess(train_df, test_df=None):\n",
        "  #.............................................................................\n",
        "  # write your codes here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #.............................................................................\n",
        "  if test_df is not None:\n",
        "      return X_train, y_train, X_test, y_test\n",
        "  else:\n",
        "      # If no test dataset is provided, return only the preprocessed training dataset\n",
        "      return X_train, y_train\n"
      ],
      "metadata": {
        "id": "eVfpNR8SHXel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (b)\n",
        "In this task, your objective is to assess the performance of various machine learning classifiers for a credit classification problem. To ensure result reproducibility, set the random_state to 42 in KFold, Decision Tree, MLP Neural Network, and Random Forest. For the MLP, configure two hidden layers with 100 and 50 hidden units, respectively, and set max_iter to 1000. For the Random Forest, set the number of trees in the forest to 100. Finally, report the accuracy achieved on the test dataset."
      ],
      "metadata": {
        "id": "hrEuzGvzPCPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "\n",
        "# Load the training and test datasets\n",
        "credit_train = pd.read_csv(\"/content/drive/MyDrive/data-mining-assignment3/datasets/credit-a-train.csv\")\n",
        "credit_test = pd.read_csv(\"/content/drive/MyDrive/data-mining-assignment3/datasets/credit-a-test.csv\")\n",
        "\n",
        "# Preprocess the datasets and split them into features (X) and target (y)\n",
        "X_train, y_train, X_test, y_test = preprocess(credit_train, credit_test)\n",
        "  #.............................................................................\n",
        "  # write your codes here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #............................................................................."
      ],
      "metadata": {
        "id": "FfasAsOYM3zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Put your discussion here**\n",
        "\n",
        "\n",
        "Discuss here.\n"
      ],
      "metadata": {
        "id": "jCssCXIfZ3OX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Question 3. (4 points)\n",
        "## Part (a)\n",
        " Use the ionosphere dataset and Run a 10-fold cross validation to evaluate classification error rate of each algorithm. Use the preprocessing function you have written in question 2. Similar to the previous question, set the random_state to 42 in KFold, Decision Tree, MLP Neural Network, and Random Forest. For the MLP, configure two hidden layers with 100 and 50 hidden units, respectively, and set max_iter to 1000. For the Random Forest, set the number of trees in the forest to 100."
      ],
      "metadata": {
        "id": "HAU0pFK4PLDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "# Load the Ionosphere dataset and preprocess it\n",
        "ionosphere_dataset = pd.read_csv(\"/content/drive/MyDrive/data-mining-assignment3/datasets/ionosphere.csv\")\n",
        "X, y = preprocess(ionosphere_dataset)\n",
        "#.............................................................................\n",
        "# write your codes here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#............................................................................."
      ],
      "metadata": {
        "id": "hk1EhL85QuBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part (b)\n",
        " Write a piece of code to perform feature selection using mutual information scores (mutual_info_classif in sklearn) on the processed ionosphere dataset (X) with its corresponding class labels (y). Your code must calculate these scores and then must select the top 5 attributes with the highest scores. Report what are these 5 selected attributes."
      ],
      "metadata": {
        "id": "KWq3AIU1Rt_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library for feature selection\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "#.............................................................................\n",
        "# write your codes here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#............................................................................."
      ],
      "metadata": {
        "id": "b9xpcyvkSnOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part (c)\n",
        "Using only the top five attributes selected by the previous feature selection method, repeat k-fold validation to report the classification errors for each classifier. Then, discuss whether the results for each classifier have improved and explore the potential reasons behind any changes."
      ],
      "metadata": {
        "id": "qZY58i2xUI4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new dataset (X_low_dim) with only the selected attributes\n",
        "X_low_dim = X[selected_attrs]\n",
        "\n",
        "#.............................................................................\n",
        "# write your codes here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#............................................................................."
      ],
      "metadata": {
        "id": "YSBbjsu2UPy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Put your discussion here**\n",
        "\n",
        "\n",
        "Discuss here.\n"
      ],
      "metadata": {
        "id": "BvFnVGhWaapb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " # Question 4. (4 points)\n",
        " Report the classification error rate on each data set and the average classification error rate of each method over all the data sets. Rank the methods according to their average classification error rate. For the top two methods (with the lowest average error rates), are their average error rates significantly different? Why? Comparing the method with the lowest average error rate and the one with the highest error rate, are their error rates significantly different? Why? Briefly discuss the results.\n",
        "\n",
        " (or result reproducibility, set the random_state to 42 in KFold, Decision Tree, MLP Neural Network, and Random Forest. Configure the MLP with two hidden layers having 100 and 50 hidden units, and set max_iter to 1000.)\n"
      ],
      "metadata": {
        "id": "XnM-sQmgWcSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#.............................................................................\n",
        "# write your codes here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#............................................................................."
      ],
      "metadata": {
        "id": "Xpb1SmryWgl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Put your discussion here**\n",
        "\n",
        "\n",
        "Discuss here.\n"
      ],
      "metadata": {
        "id": "IPoKqDTuachd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Question 5. (14 points)"
      ],
      "metadata": {
        "id": "7k7a5YplXIdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Please include your code for this section below. You may need to define various functions,\n",
        "#such as a preprocessing function, and incorporate them into your code.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sQ5I8_5RajHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For question 5, your detailed report should be presented in a separate PDF file.**"
      ],
      "metadata": {
        "id": "DkVpfBdrdoXr"
      }
    }
  ]
}